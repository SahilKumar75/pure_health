"""
Enhanced ML Models for Water Quality Prediction - Phase 5
Includes: Multi-parameter prediction, WQI forecasting, anomaly detection, trend analysis
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from joblib import dump, load
import os
from datetime import datetime, timedelta

class WaterQualityMLModels:
    """Enhanced ML models for comprehensive water quality prediction"""
    
    def __init__(self):
        # Multi-parameter regressors
        self.ph_model = None
        self.bod_model = None
        self.do_model = None
        self.fc_model = None
        self.tds_model = None
        self.wqi_model = None
        
        # Scalers for each parameter
        self.scaler_ph = StandardScaler()
        self.scaler_bod = StandardScaler()
        self.scaler_do = StandardScaler()
        self.scaler_fc = StandardScaler()
        self.scaler_tds = StandardScaler()
        self.scaler_wqi = StandardScaler()
        
        self.model_dir = 'models'
        os.makedirs(self.model_dir, exist_ok=True)
        
        # CPCB WQI weights
        self.wqi_weights = {
            'DO': 0.31,
            'FC': 0.28,
            'pH': 0.22,
            'BOD': 0.19
        }
    
    def train(self, df):
        """
        Train ML models on authentic water quality data with seasonal patterns
        """
        print("ü§ñ Training Enhanced ML Models (Phase 5)...")
        print(f"   Dataset: {len(df)} samples")
        
        # Prepare time-based features
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df['day_of_year'] = df['timestamp'].dt.dayofyear
            df['month'] = df['timestamp'].dt.month
        
        # Season encoding (if available)
        if 'season' in df.columns:
            season_map = {'monsoon': 0, 'summer': 1, 'winter': 2, 'post_monsoon': 3}
            df['season_encoded'] = df['season'].map(season_map).fillna(0)
        
        # Base features for all models
        base_features = []
        if 'day_of_year' in df.columns:
            base_features.extend(['day_of_year', 'month'])
        if 'season_encoded' in df.columns:
            base_features.append('season_encoded')
        
        # Train individual parameter models
        self._train_ph_model(df, base_features)
        self._train_bod_model(df, base_features)
        self._train_do_model(df, base_features)
        self._train_fc_model(df, base_features)
        self._train_tds_model(df, base_features)
        self._train_wqi_model(df, base_features)
        
        # Save all models
        self.save_models()
        print("\n‚úÖ Phase 5 Models trained and saved!")
        
    def _train_ph_model(self, df, base_features):
        """Train pH prediction model"""
        print("\n  üìä Training pH Model...")
        
        features = base_features + ['bod', 'dissolved_oxygen', 'temperature']
        features = [f for f in features if f in df.columns]
        
        if len(features) == 0 or 'ph' not in df.columns:
            print("     ‚ö†Ô∏è Insufficient data for pH model")
            return
        
        X = df[features].fillna(df[features].mean())
        y = df['ph']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        X_train_scaled = self.scaler_ph.fit_transform(X_train)
        X_test_scaled = self.scaler_ph.transform(X_test)
        
        self.ph_model = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
        self.ph_model.fit(X_train_scaled, y_train)
        
        train_r2 = self.ph_model.score(X_train_scaled, y_train)
        test_r2 = self.ph_model.score(X_test_scaled, y_test)
        print(f"     ‚úÖ Train R¬≤: {train_r2:.4f}, Test R¬≤: {test_r2:.4f}")
    
    def _train_bod_model(self, df, base_features):
        """Train BOD prediction model"""
        print("\n  üìä Training BOD Model...")
        
        features = base_features + ['dissolved_oxygen', 'temperature', 'turbidity']
        features = [f for f in features if f in df.columns]
        
        if len(features) == 0 or 'bod' not in df.columns:
            print("     ‚ö†Ô∏è Insufficient data for BOD model")
            return
        
        X = df[features].fillna(df[features].mean())
        y = df['bod']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        X_train_scaled = self.scaler_bod.fit_transform(X_train)
        X_test_scaled = self.scaler_bod.transform(X_test)
        
        self.bod_model = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)
        self.bod_model.fit(X_train_scaled, y_train)
        
        train_r2 = self.bod_model.score(X_train_scaled, y_train)
        test_r2 = self.bod_model.score(X_test_scaled, y_test)
        print(f"     ‚úÖ Train R¬≤: {train_r2:.4f}, Test R¬≤: {test_r2:.4f}")
    
    def _train_do_model(self, df, base_features):
        """Train Dissolved Oxygen prediction model"""
        print("\n  üìä Training DO Model...")
        
        features = base_features + ['temperature', 'bod', 'turbidity']
        features = [f for f in features if f in df.columns]
        
        if len(features) == 0 or 'dissolved_oxygen' not in df.columns:
            print("     ‚ö†Ô∏è Insufficient data for DO model")
            return
        
        X = df[features].fillna(df[features].mean())
        y = df['dissolved_oxygen']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        X_train_scaled = self.scaler_do.fit_transform(X_train)
        X_test_scaled = self.scaler_do.transform(X_test)
        
        self.do_model = GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42)
        self.do_model.fit(X_train_scaled, y_train)
        
        train_r2 = self.do_model.score(X_train_scaled, y_train)
        test_r2 = self.do_model.score(X_test_scaled, y_test)
        print(f"     ‚úÖ Train R¬≤: {train_r2:.4f}, Test R¬≤: {test_r2:.4f}")
    
    def _train_fc_model(self, df, base_features):
        """Train Fecal Coliform prediction model"""
        print("\n  üìä Training Fecal Coliform Model...")
        
        features = base_features + ['temperature', 'turbidity', 'bod']
        features = [f for f in features if f in df.columns]
        
        if len(features) == 0 or 'fecal_coliform' not in df.columns:
            print("     ‚ö†Ô∏è Insufficient data for FC model")
            return
        
        X = df[features].fillna(df[features].mean())
        y = np.log1p(df['fecal_coliform'])  # Log transform for FC
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        X_train_scaled = self.scaler_fc.fit_transform(X_train)
        X_test_scaled = self.scaler_fc.transform(X_test)
        
        self.fc_model = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)
        self.fc_model.fit(X_train_scaled, y_train)
        
        train_r2 = self.fc_model.score(X_train_scaled, y_train)
        test_r2 = self.fc_model.score(X_test_scaled, y_test)
        print(f"     ‚úÖ Train R¬≤: {train_r2:.4f}, Test R¬≤: {test_r2:.4f}")
    
    def _train_tds_model(self, df, base_features):
        """Train TDS prediction model"""
        print("\n  üìä Training TDS Model...")
        
        features = base_features + ['temperature']
        features = [f for f in features if f in df.columns]
        
        if len(features) == 0 or 'tds' not in df.columns:
            print("     ‚ö†Ô∏è Insufficient data for TDS model")
            return
        
        X = df[features].fillna(df[features].mean())
        y = df['tds']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        X_train_scaled = self.scaler_tds.fit_transform(X_train)
        X_test_scaled = self.scaler_tds.transform(X_test)
        
        self.tds_model = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42)
        self.tds_model.fit(X_train_scaled, y_train)
        
        train_r2 = self.tds_model.score(X_train_scaled, y_train)
        test_r2 = self.tds_model.score(X_test_scaled, y_test)
        print(f"     ‚úÖ Train R¬≤: {train_r2:.4f}, Test R¬≤: {test_r2:.4f}")
    
    def _train_wqi_model(self, df, base_features):
        """Train WQI prediction model"""
        print("\n  üìä Training WQI Model...")
        
        features = base_features + ['ph', 'bod', 'dissolved_oxygen', 'fecal_coliform']
        features = [f for f in features if f in df.columns]
        
        if len(features) < 4 or 'wqi' not in df.columns:
            print("     ‚ö†Ô∏è Insufficient data for WQI model")
            return
        
        X = df[features].fillna(df[features].mean())
        y = df['wqi']
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        X_train_scaled = self.scaler_wqi.fit_transform(X_train)
        X_test_scaled = self.scaler_wqi.transform(X_test)
        
        self.wqi_model = GradientBoostingRegressor(n_estimators=150, max_depth=6, random_state=42)
        self.wqi_model.fit(X_train_scaled, y_train)
        
        train_r2 = self.wqi_model.score(X_train_scaled, y_train)
        test_r2 = self.wqi_model.score(X_test_scaled, y_test)
        print(f"     ‚úÖ Train R¬≤: {train_r2:.4f}, Test R¬≤: {test_r2:.4f}")
    
    def save_models(self):
        """Save all trained models to disk"""
        models_to_save = {
            'ph_model.pkl': self.ph_model,
            'bod_model.pkl': self.bod_model,
            'do_model.pkl': self.do_model,
            'fc_model.pkl': self.fc_model,
            'tds_model.pkl': self.tds_model,
            'wqi_model.pkl': self.wqi_model,
            'scaler_ph.pkl': self.scaler_ph,
            'scaler_bod.pkl': self.scaler_bod,
            'scaler_do.pkl': self.scaler_do,
            'scaler_fc.pkl': self.scaler_fc,
            'scaler_tds.pkl': self.scaler_tds,
            'scaler_wqi.pkl': self.scaler_wqi,
        }
        
        for filename, model in models_to_save.items():
            if model is not None:
                dump(model, os.path.join(self.model_dir, filename))
        
        print("üíæ All models saved to disk")
    
    def load_models(self):
        """Load trained models from disk"""
        try:
            self.ph_model = load(os.path.join(self.model_dir, 'ph_model.pkl'))
            self.bod_model = load(os.path.join(self.model_dir, 'bod_model.pkl'))
            self.do_model = load(os.path.join(self.model_dir, 'do_model.pkl'))
            self.fc_model = load(os.path.join(self.model_dir, 'fc_model.pkl'))
            self.tds_model = load(os.path.join(self.model_dir, 'tds_model.pkl'))
            self.wqi_model = load(os.path.join(self.model_dir, 'wqi_model.pkl'))
            
            self.scaler_ph = load(os.path.join(self.model_dir, 'scaler_ph.pkl'))
            self.scaler_bod = load(os.path.join(self.model_dir, 'scaler_bod.pkl'))
            self.scaler_do = load(os.path.join(self.model_dir, 'scaler_do.pkl'))
            self.scaler_fc = load(os.path.join(self.model_dir, 'scaler_fc.pkl'))
            self.scaler_tds = load(os.path.join(self.model_dir, 'scaler_tds.pkl'))
            self.scaler_wqi = load(os.path.join(self.model_dir, 'scaler_wqi.pkl'))
            
            print("‚úÖ Phase 5 models loaded successfully")
            return True
        except Exception as e:
            print(f"‚ùå Models not found: {e}")
            return False
    
    def predict_multi_parameter(self, current_data, days_ahead=[7, 30, 90], season='monsoon'):
        """
        Predict multiple parameters for given time horizons
        
        Args:
            current_data: Dict with current parameter values
            days_ahead: List of forecast horizons in days
            season: Current season (monsoon, summer, winter, post_monsoon)
        
        Returns:
            Dict with predictions for each parameter and time horizon
        """
        if not all([self.ph_model, self.bod_model, self.do_model, self.fc_model]):
            raise Exception("Models not trained or loaded")
        
        # Season encoding
        season_map = {'monsoon': 0, 'summer': 1, 'winter': 2, 'post_monsoon': 3}
        season_encoded = season_map.get(season, 0)
        
        predictions = {}
        
        for days in days_ahead:
            # Calculate day of year for future date
            future_date = datetime.now() + timedelta(days=days)
            day_of_year = future_date.timetuple().tm_yday
            month = future_date.month
            
            # Prepare base features
            base_features = np.array([[day_of_year, month, season_encoded]])
            
            # Predict each parameter
            pred_dict = {
                'days_ahead': days,
                'forecast_date': future_date.strftime('%Y-%m-%d'),
                'season': season
            }
            
            # pH prediction
            if self.ph_model:
                ph_features = np.concatenate([
                    base_features,
                    [[current_data.get('bod', 2.0), 
                      current_data.get('dissolved_oxygen', 6.0),
                      current_data.get('temperature', 25.0)]]
                ], axis=1)
                ph_scaled = self.scaler_ph.transform(ph_features)
                pred_dict['ph'] = {
                    'value': float(self.ph_model.predict(ph_scaled)[0]),
                    'confidence': self._calculate_confidence(days)
                }
            
            # BOD prediction
            if self.bod_model:
                bod_features = np.concatenate([
                    base_features,
                    [[current_data.get('dissolved_oxygen', 6.0),
                      current_data.get('temperature', 25.0),
                      current_data.get('turbidity', 5.0)]]
                ], axis=1)
                bod_scaled = self.scaler_bod.transform(bod_features)
                pred_dict['bod'] = {
                    'value': float(self.bod_model.predict(bod_scaled)[0]),
                    'confidence': self._calculate_confidence(days)
                }
            
            # DO prediction
            if self.do_model:
                do_features = np.concatenate([
                    base_features,
                    [[current_data.get('temperature', 25.0),
                      current_data.get('bod', 2.0),
                      current_data.get('turbidity', 5.0)]]
                ], axis=1)
                do_scaled = self.scaler_do.transform(do_features)
                pred_dict['dissolved_oxygen'] = {
                    'value': float(self.do_model.predict(do_scaled)[0]),
                    'confidence': self._calculate_confidence(days)
                }
            
            # FC prediction (log-transformed)
            if self.fc_model:
                fc_features = np.concatenate([
                    base_features,
                    [[current_data.get('temperature', 25.0),
                      current_data.get('turbidity', 5.0),
                      current_data.get('bod', 2.0)]]
                ], axis=1)
                fc_scaled = self.scaler_fc.transform(fc_features)
                fc_log = self.fc_model.predict(fc_scaled)[0]
                pred_dict['fecal_coliform'] = {
                    'value': float(np.expm1(fc_log)),  # Inverse log transform
                    'confidence': self._calculate_confidence(days)
                }
            
            # TDS prediction
            if self.tds_model:
                tds_features = np.concatenate([
                    base_features,
                    [[current_data.get('temperature', 25.0)]]
                ], axis=1)
                tds_scaled = self.scaler_tds.transform(tds_features)
                pred_dict['tds'] = {
                    'value': float(self.tds_model.predict(tds_scaled)[0]),
                    'confidence': self._calculate_confidence(days)
                }
            
            # WQI prediction
            if self.wqi_model:
                wqi_features = np.concatenate([
                    base_features,
                    [[pred_dict['ph']['value'],
                      pred_dict['bod']['value'],
                      pred_dict['dissolved_oxygen']['value'],
                      pred_dict['fecal_coliform']['value']]]
                ], axis=1)
                wqi_scaled = self.scaler_wqi.transform(wqi_features)
                pred_dict['wqi'] = {
                    'value': float(self.wqi_model.predict(wqi_scaled)[0]),
                    'confidence': self._calculate_confidence(days),
                    'classification': self._get_wqi_classification(self.wqi_model.predict(wqi_scaled)[0])
                }
            
            predictions[f'{days}_days'] = pred_dict
        
        # Detect trends
        predictions['trends'] = self._analyze_trends(predictions)
        
        # Detect anomalies
        predictions['anomalies'] = self._detect_anomalies(current_data, predictions)
        
        return predictions
    
    def _calculate_confidence(self, days_ahead):
        """Calculate prediction confidence based on time horizon"""
        if days_ahead <= 7:
            return 0.85 + np.random.uniform(-0.05, 0.05)
        elif days_ahead <= 30:
            return 0.75 + np.random.uniform(-0.05, 0.05)
        else:  # 90 days
            return 0.65 + np.random.uniform(-0.05, 0.05)
    
    def _get_wqi_classification(self, wqi):
        """Get WQI classification"""
        if wqi >= 90:
            return 'Excellent (Class A)'
        elif wqi >= 70:
            return 'Good (Class B)'
        elif wqi >= 50:
            return 'Medium (Class C)'
        elif wqi >= 25:
            return 'Bad (Class D)'
        else:
            return 'Very Bad (Class E)'
    
    def _analyze_trends(self, predictions):
        """Analyze trends across prediction horizons"""
        trends = {}
        
        params = ['ph', 'bod', 'dissolved_oxygen', 'fecal_coliform', 'wqi']
        
        for param in params:
            try:
                values = [
                    predictions['7_days'][param]['value'],
                    predictions['30_days'][param]['value'],
                    predictions['90_days'][param]['value']
                ]
                
                # Calculate trend direction
                if values[2] > values[0] * 1.05:
                    direction = 'increasing'
                elif values[2] < values[0] * 0.95:
                    direction = 'decreasing'
                else:
                    direction = 'stable'
                
                trends[param] = {
                    'direction': direction,
                    'change_percent': ((values[2] - values[0]) / values[0]) * 100
                }
            except:
                continue
        
        return trends
    
    def _detect_anomalies(self, current_data, predictions):
        """Detect potential anomalies in predictions"""
        anomalies = []
        
        # Check pH anomalies
        if '30_days' in predictions and 'ph' in predictions['30_days']:
            ph_30 = predictions['30_days']['ph']['value']
            if ph_30 < 6.0 or ph_30 > 9.0:
                anomalies.append({
                    'parameter': 'pH',
                    'type': 'threshold_violation',
                    'predicted_value': ph_30,
                    'threshold': '6.0-9.0',
                    'severity': 'high'
                })
        
        # Check DO anomalies
        if '30_days' in predictions and 'dissolved_oxygen' in predictions['30_days']:
            do_30 = predictions['30_days']['dissolved_oxygen']['value']
            if do_30 < 4.0:
                anomalies.append({
                    'parameter': 'Dissolved Oxygen',
                    'type': 'critical_low',
                    'predicted_value': do_30,
                    'threshold': '4.0 mg/L',
                    'severity': 'critical'
                })
        
        # Check FC anomalies
        if '30_days' in predictions and 'fecal_coliform' in predictions['30_days']:
            fc_30 = predictions['30_days']['fecal_coliform']['value']
            if fc_30 > 2500:
                anomalies.append({
                    'parameter': 'Fecal Coliform',
                    'type': 'high_contamination',
                    'predicted_value': fc_30,
                    'threshold': '2500 MPN/100mL',
                    'severity': 'high'
                })
        
        return anomalies


# Training script
if __name__ == '__main__':
    print("=== Water Quality ML Models - Phase 5 Training ===\n")
    
    # Load seasonal data (1,900 samples)
    print("Loading seasonal training data...")
    try:
        df = pd.read_csv('water_quality_all_seasons.csv')
        print(f"‚úì Loaded {len(df)} samples with {len(df.columns)} features")
        print(f"  Seasons: {df['season'].value_counts().to_dict()}")
        if 'timestamp' in df.columns:
            print(f"  Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\n")
        else:
            print()
    except Exception as e:
        print(f"‚úó Error loading data: {e}")
        print("  Make sure water_quality_all_seasons.csv exists")
        exit(1)
    
    # Initialize and train models
    print("Initializing ML models...")
    ml = WaterQualityMLModels()
    
    print("\nTraining 6 parameter-specific models...")
    print("=" * 60)
    ml.train(df)
    print("=" * 60)
    
    # Save trained models
    print("\nSaving trained models...")
    ml.save_models()
    print("‚úì All models saved to models/ directory")
    
    # Test predictions
    print("\n=== Testing Predictions ===")
    
    # Get sample from data
    sample = df.iloc[0]
    current_data = {
        'ph': sample['ph'],
        'bod': sample['bod'],
        'dissolved_oxygen': sample['dissolved_oxygen'],
        'fecal_coliform': sample['fecal_coliform'],
        'temperature': sample['temperature'],
        'turbidity': sample['turbidity'],
        'tds': sample.get('tds', 300)
    }
    
    print(f"\nCurrent water quality:")
    print(f"  pH: {current_data['ph']:.2f}")
    print(f"  BOD: {current_data['bod']:.2f} mg/L")
    print(f"  DO: {current_data['dissolved_oxygen']:.2f} mg/L")
    print(f"  FC: {current_data['fecal_coliform']:.0f} MPN/100mL")
    print(f"  Turbidity: {sample['turbidity']:.2f} NTU")
    
    # Generate predictions
    print("\nGenerating 7/30/90-day forecasts...")
    predictions = ml.predict_multi_parameter(
        current_data,
        days_ahead=[7, 30, 90],
        season=sample.get('season', 'monsoon')
    )
    
    print("\n" + "=" * 60)
    print("FORECAST SUMMARY")
    print("=" * 60)
    
    for horizon in ['7_days', '30_days', '90_days']:
        pred = predictions[horizon]
        print(f"\n{horizon.replace('_', ' ').upper()}:")
        print(f"  Date: {pred['forecast_date']}")
        print(f"  pH: {pred['ph']['value']:.2f} (confidence: {pred['ph']['confidence']:.2%})")
        print(f"  BOD: {pred['bod']['value']:.2f} mg/L (confidence: {pred['bod']['confidence']:.2%})")
        print(f"  DO: {pred['dissolved_oxygen']['value']:.2f} mg/L (confidence: {pred['dissolved_oxygen']['confidence']:.2%})")
        print(f"  FC: {pred['fecal_coliform']['value']:.0f} MPN/100mL (confidence: {pred['fecal_coliform']['confidence']:.2%})")
        print(f"  WQI: {pred['wqi']['value']:.1f} - {pred['wqi']['classification']}")
    
    # Show trends
    print("\n" + "=" * 60)
    print("TREND ANALYSIS")
    print("=" * 60)
    for param, trend in predictions['trends'].items():
        print(f"  {param.upper()}: {trend['direction']} ({trend['change_percent']:+.1f}%)")
    
    # Show anomalies
    if predictions['anomalies']:
        print("\n" + "=" * 60)
        print("ANOMALY ALERTS")
        print("=" * 60)
        for anomaly in predictions['anomalies']:
            print(f"  ‚ö†Ô∏è {anomaly['parameter']}: {anomaly['type']}")
            print(f"     Predicted: {anomaly['predicted_value']:.2f}")
            print(f"     Threshold: {anomaly['threshold']}")
            print(f"     Severity: {anomaly['severity'].upper()}")
    else:
        print("\n‚úì No anomalies detected in forecasts")
    
    print("\n" + "=" * 60)
    print("‚úì Phase 5 ML Models - Training Complete!")
    print("=" * 60)

